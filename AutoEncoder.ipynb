{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abstract-candy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from embeddings.dataloader import TheDataSet\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "encoder_training_epochs = 150\n",
    "# dataset_file = 'data_ae_train/fulldata.npy'\n",
    "dataset_file = 'data/fulldata.npy'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter(f\"runs/autoencoder_experiment_{encoder_training_epochs}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "divided-metropolitan",
   "metadata": {},
   "source": [
    "## AutoEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "removed-giant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting embeddings/autoencoder.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile embeddings/autoencoder.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(in_features=num_features, out_features=256 , bias=True),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=256, out_features=128, bias=True),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=128, out_features=32, bias=True),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(in_features=32, out_features=128, bias=True),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=128, out_features=256, bias=True),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(in_features=256, out_features=num_features, bias=True ),\n",
    "            #nn.Sigmoid()\n",
    "            nn.Tanh()\n",
    "#             nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from embeddings.autoencoder import Autoencoder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "joined-cornell",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "historic-banks",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataset, num_epochs=5, batch_size=64, learning_rate=1e-3):\n",
    "    torch.manual_seed(42)\n",
    "    # criterion = nn.MSELoss() # mean square error loss\n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                 lr=learning_rate, \n",
    "                                 weight_decay=1e-5) # <--\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    outputs = []\n",
    "    losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for X, y in train_loader:\n",
    "            recon = model(X.float())\n",
    "            loss = criterion(recon, X.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        print('Epoch:{}, Loss:{:.4f}'.format(epoch+1, float(loss)))\n",
    "        losses.append(float(loss))\n",
    "        writer.add_scalar(f'training loss {num_epochs}',\n",
    "                            loss,\n",
    "                            epoch * len(train_loader))\n",
    "    return outputs, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silver-congo",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=338, out_features=256, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (3): Tanh()\n",
      "    (4): Linear(in_features=128, out_features=32, bias=True)\n",
      "    (5): Tanh()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=128, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): Tanh()\n",
      "    (4): Linear(in_features=256, out_features=338, bias=True)\n",
      "    (5): Tanh()\n",
      "  )\n",
      ")\n",
      "Epoch:1, Loss:0.1464\n",
      "Epoch:2, Loss:0.1419\n",
      "Epoch:3, Loss:0.1555\n",
      "Epoch:4, Loss:0.1251\n",
      "Epoch:5, Loss:0.1205\n",
      "Epoch:6, Loss:0.1244\n",
      "Epoch:7, Loss:0.1193\n",
      "Epoch:8, Loss:0.1280\n",
      "Epoch:9, Loss:0.1262\n",
      "Epoch:10, Loss:0.1352\n",
      "Epoch:11, Loss:0.1292\n",
      "Epoch:12, Loss:0.0952\n",
      "Epoch:13, Loss:0.1031\n",
      "Epoch:14, Loss:0.1181\n",
      "Epoch:15, Loss:0.1095\n",
      "Epoch:16, Loss:0.1405\n",
      "Epoch:17, Loss:0.1126\n",
      "Epoch:18, Loss:0.1080\n",
      "Epoch:19, Loss:0.1257\n",
      "Epoch:20, Loss:0.1178\n",
      "Epoch:21, Loss:0.1115\n",
      "Epoch:22, Loss:0.1134\n",
      "Epoch:23, Loss:0.0970\n",
      "Epoch:24, Loss:0.1075\n",
      "Epoch:25, Loss:0.1025\n",
      "Epoch:26, Loss:0.1070\n",
      "Epoch:27, Loss:0.1259\n",
      "Epoch:28, Loss:0.1336\n",
      "Epoch:29, Loss:0.1110\n",
      "Epoch:30, Loss:0.1033\n",
      "Epoch:31, Loss:0.1124\n",
      "Epoch:32, Loss:0.1044\n",
      "Epoch:33, Loss:0.1076\n",
      "Epoch:34, Loss:0.0895\n",
      "Epoch:35, Loss:0.1063\n",
      "Epoch:36, Loss:0.0933\n",
      "Epoch:37, Loss:0.1011\n",
      "Epoch:38, Loss:0.0905\n",
      "Epoch:39, Loss:0.1153\n",
      "Epoch:40, Loss:0.0878\n",
      "Epoch:41, Loss:0.0813\n",
      "Epoch:42, Loss:0.0959\n",
      "Epoch:43, Loss:0.2075\n",
      "Epoch:44, Loss:0.0991\n",
      "Epoch:45, Loss:0.0890\n",
      "Epoch:46, Loss:0.1156\n",
      "Epoch:47, Loss:0.0785\n",
      "Epoch:48, Loss:0.0934\n",
      "Epoch:49, Loss:0.1085\n",
      "Epoch:50, Loss:0.1020\n",
      "Epoch:51, Loss:0.0935\n",
      "Epoch:52, Loss:0.1055\n",
      "Epoch:53, Loss:0.0967\n",
      "Epoch:54, Loss:0.0900\n",
      "Epoch:55, Loss:0.0985\n",
      "Epoch:56, Loss:0.1032\n",
      "Epoch:57, Loss:0.0932\n",
      "Epoch:58, Loss:0.1174\n",
      "Epoch:59, Loss:0.0858\n",
      "Epoch:60, Loss:0.1003\n",
      "Epoch:61, Loss:0.0984\n",
      "Epoch:62, Loss:0.1067\n",
      "Epoch:63, Loss:0.0861\n",
      "Epoch:64, Loss:0.0861\n",
      "Epoch:65, Loss:0.1189\n",
      "Epoch:66, Loss:0.0860\n",
      "Epoch:67, Loss:0.1161\n",
      "Epoch:68, Loss:0.0980\n",
      "Epoch:69, Loss:0.0903\n",
      "Epoch:70, Loss:0.0970\n",
      "Epoch:71, Loss:0.2061\n",
      "Epoch:72, Loss:0.0914\n",
      "Epoch:73, Loss:0.0944\n",
      "Epoch:74, Loss:0.0876\n",
      "Epoch:75, Loss:0.0952\n",
      "Epoch:76, Loss:0.0823\n",
      "Epoch:77, Loss:0.0914\n",
      "Epoch:78, Loss:0.1057\n",
      "Epoch:79, Loss:0.0854\n",
      "Epoch:80, Loss:0.0924\n",
      "Epoch:81, Loss:0.1319\n",
      "Epoch:82, Loss:0.0816\n",
      "Epoch:83, Loss:0.1014\n",
      "Epoch:84, Loss:0.0932\n",
      "Epoch:85, Loss:0.0901\n",
      "Epoch:86, Loss:0.1018\n",
      "Epoch:87, Loss:0.1012\n",
      "Epoch:88, Loss:0.1149\n",
      "Epoch:89, Loss:0.0844\n",
      "Epoch:90, Loss:0.0986\n",
      "Epoch:91, Loss:0.0974\n",
      "Epoch:92, Loss:0.0870\n",
      "Epoch:93, Loss:0.0968\n",
      "Epoch:94, Loss:0.1071\n",
      "Epoch:95, Loss:0.0985\n",
      "Epoch:96, Loss:0.0936\n",
      "Epoch:97, Loss:0.1366\n",
      "Epoch:98, Loss:0.0892\n",
      "Epoch:99, Loss:0.0844\n",
      "Epoch:100, Loss:0.1021\n",
      "Epoch:101, Loss:0.1095\n",
      "Epoch:102, Loss:0.1016\n",
      "Epoch:103, Loss:0.0819\n",
      "Epoch:104, Loss:0.0956\n",
      "Epoch:105, Loss:0.0823\n",
      "Epoch:106, Loss:0.1098\n",
      "Epoch:107, Loss:0.0910\n",
      "Epoch:108, Loss:0.0818\n",
      "Epoch:109, Loss:0.0807\n",
      "Epoch:110, Loss:0.0952\n",
      "Epoch:111, Loss:0.1030\n",
      "Epoch:112, Loss:0.0713\n",
      "Epoch:113, Loss:0.0818\n",
      "Epoch:114, Loss:0.1007\n",
      "Epoch:115, Loss:0.0824\n",
      "Epoch:116, Loss:0.0893\n",
      "Epoch:117, Loss:0.1008\n",
      "Epoch:118, Loss:0.0805\n",
      "Epoch:119, Loss:0.1211\n",
      "Epoch:120, Loss:0.1622\n",
      "Epoch:121, Loss:0.1231\n",
      "Epoch:122, Loss:0.0938\n",
      "Epoch:123, Loss:0.0917\n",
      "Epoch:124, Loss:0.1177\n"
     ]
    }
   ],
   "source": [
    "dataset = TheDataSet(datafile=dataset_file, pad_to_360=False)\n",
    "model = Autoencoder(num_features=dataset.num_features())\n",
    "print(model)\n",
    "max_epochs = encoder_training_epochs\n",
    "outputs, losses = train(model, dataset=dataset, num_epochs=max_epochs, batch_size = 64, learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romance-surveillance",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "\n",
    "# x = np.linspace(0, 10, 1000)\n",
    "ax.plot(losses);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# persist the encoder\n",
    "\n",
    "\n",
    "with open('data/autoencoder.pic', 'bw') as f:\n",
    "    torch.save(model, f, pickle_protocol=4)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open('data/autoencoder.pic', 'rb') as f:\n",
    "    autoencoder = torch.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-4b3cae3f",
   "language": "python",
   "display_name": "PyCharm (resistance-prediction)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}