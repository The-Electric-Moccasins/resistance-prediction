{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abstract-candy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from embeddings.dataloader import TheDataSet\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "encoder_training_epochs = 100"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter(f\"runs/autoencoder_experiment_{encoder_training_epochs}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "divided-metropolitan",
   "metadata": {},
   "source": [
    "## AutoEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "removed-giant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting embeddings/autoencoder.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile embeddings/autoencoder.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(in_features=num_features, out_features=256 , bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256, out_features=128, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=64, bias=True),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(in_features=64, out_features=128, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=256, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256, out_features=num_features, bias=True ),\n",
    "            #nn.Sigmoid()\n",
    "            nn.ReLU()\n",
    "#             nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from embeddings.autoencoder import Autoencoder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "joined-cornell",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "historic-banks",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataset, num_epochs=5, batch_size=64, learning_rate=1e-3):\n",
    "    torch.manual_seed(42)\n",
    "    # criterion = nn.MSELoss() # mean square error loss\n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                 lr=learning_rate, \n",
    "                                 weight_decay=1e-5) # <--\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    outputs = []\n",
    "    losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for X, y in train_loader:\n",
    "            recon = model(X.float())\n",
    "            loss = criterion(recon, X.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        print('Epoch:{}, Loss:{:.4f}'.format(epoch+1, float(loss)))\n",
    "        losses.append(float(loss))\n",
    "        writer.add_scalar(f'training loss {num_epochs}',\n",
    "                            loss,\n",
    "                            epoch * len(train_loader))\n",
    "    return outputs, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silver-congo",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=360, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=360, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ")\n",
      "Epoch:1, Loss:0.1395\n",
      "Epoch:2, Loss:0.1503\n",
      "Epoch:3, Loss:0.1530\n",
      "Epoch:4, Loss:0.1489\n",
      "Epoch:5, Loss:0.1361\n",
      "Epoch:6, Loss:0.1234\n",
      "Epoch:7, Loss:0.1330\n",
      "Epoch:8, Loss:0.1136\n",
      "Epoch:9, Loss:0.1506\n",
      "Epoch:10, Loss:0.1648\n",
      "Epoch:11, Loss:0.1241\n",
      "Epoch:12, Loss:0.1063\n",
      "Epoch:13, Loss:0.1073\n",
      "Epoch:14, Loss:0.1968\n",
      "Epoch:15, Loss:0.1234\n",
      "Epoch:16, Loss:0.1590\n",
      "Epoch:17, Loss:0.1156\n",
      "Epoch:18, Loss:0.1090\n",
      "Epoch:19, Loss:0.1277\n",
      "Epoch:20, Loss:0.1432\n",
      "Epoch:21, Loss:0.1037\n",
      "Epoch:22, Loss:0.1262\n",
      "Epoch:23, Loss:0.1518\n",
      "Epoch:24, Loss:0.1455\n",
      "Epoch:25, Loss:0.1076\n",
      "Epoch:26, Loss:0.1257\n",
      "Epoch:27, Loss:0.1182\n",
      "Epoch:28, Loss:0.1564\n",
      "Epoch:29, Loss:0.1297\n",
      "Epoch:30, Loss:0.1014\n",
      "Epoch:31, Loss:0.1216\n",
      "Epoch:32, Loss:0.1202\n",
      "Epoch:33, Loss:0.1231\n",
      "Epoch:34, Loss:0.1232\n",
      "Epoch:35, Loss:0.1146\n",
      "Epoch:36, Loss:0.1071\n",
      "Epoch:37, Loss:0.1046\n",
      "Epoch:38, Loss:0.0995\n",
      "Epoch:39, Loss:0.1114\n",
      "Epoch:40, Loss:0.1389\n",
      "Epoch:41, Loss:0.1018\n",
      "Epoch:42, Loss:0.0784\n",
      "Epoch:43, Loss:0.1133\n",
      "Epoch:44, Loss:0.1187\n",
      "Epoch:45, Loss:0.1264\n",
      "Epoch:46, Loss:0.1013\n",
      "Epoch:47, Loss:0.1002\n",
      "Epoch:48, Loss:0.0974\n",
      "Epoch:49, Loss:0.1489\n",
      "Epoch:50, Loss:0.1218\n",
      "Epoch:51, Loss:0.1302\n",
      "Epoch:52, Loss:0.1116\n",
      "Epoch:53, Loss:0.1198\n",
      "Epoch:54, Loss:0.1113\n",
      "Epoch:55, Loss:0.0993\n",
      "Epoch:56, Loss:0.1370\n",
      "Epoch:57, Loss:0.1300\n",
      "Epoch:58, Loss:0.2417\n",
      "Epoch:59, Loss:0.0965\n",
      "Epoch:60, Loss:0.1201\n",
      "Epoch:61, Loss:0.1026\n",
      "Epoch:62, Loss:0.1143\n",
      "Epoch:63, Loss:0.0882\n",
      "Epoch:64, Loss:0.0923\n",
      "Epoch:65, Loss:0.1270\n",
      "Epoch:66, Loss:0.0824\n",
      "Epoch:67, Loss:0.1195\n",
      "Epoch:68, Loss:0.1517\n",
      "Epoch:69, Loss:0.1100\n",
      "Epoch:70, Loss:0.1095\n",
      "Epoch:71, Loss:0.6131\n",
      "Epoch:72, Loss:0.1259\n",
      "Epoch:73, Loss:0.1217\n",
      "Epoch:74, Loss:0.0790\n",
      "Epoch:75, Loss:0.1155\n",
      "Epoch:76, Loss:0.0744\n",
      "Epoch:77, Loss:0.0902\n",
      "Epoch:78, Loss:0.1409\n",
      "Epoch:79, Loss:0.0714\n",
      "Epoch:80, Loss:0.1138\n",
      "Epoch:81, Loss:0.1609\n"
     ]
    }
   ],
   "source": [
    "dataset = TheDataSet(datafile='data/fulldata.npy')\n",
    "model = Autoencoder(num_features=dataset.num_features())\n",
    "print(model)\n",
    "max_epochs = encoder_training_epochs\n",
    "outputs, losses = train(model, dataset=dataset, num_epochs=max_epochs, batch_size = 32, learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romance-surveillance",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "\n",
    "# x = np.linspace(0, 10, 1000)\n",
    "ax.plot(losses);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# persist the encoder\n",
    "\n",
    "\n",
    "with open('data/autoencoder.pic', 'bw') as f:\n",
    "    torch.save(model, f, pickle_protocol=4)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open('data/autoencoder.pic', 'rb') as f:\n",
    "    autoencoder = torch.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-4b3cae3f",
   "language": "python",
   "display_name": "PyCharm (resistance-prediction)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}